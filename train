import datetime
import os
import sys
import argparse
import logging

import cv2
import tensorflow as tf
from qkeras import *
from keras.layers import *
from keras.models import Model
from keras.optimizers import Adam
from keras import losses
import torch
import numpy
import torch.utils.data

import tensorboardX
from torch.utils.tensorboard import SummaryWriter


from dataprocessing import GraspDatasetBase
from loss_function import *
from pretrain import *
from evaluation import *


def parse_args():
    parser = argparse.ArgumentParser(description='Train GG-CNN')

    # Network
    parser.add_argument('--network', type=str, default='ggcnn', help='Network Name in .models')

    # Dataset & Data & Training
    parser.add_argument('--dataset', type=str, help='Dataset Name ("cornell" or "jaquard")')
    parser.add_argument('--dataset-path', type=str, help='Path to dataset')
    parser.add_argument('--use-depth', type=int, default=1, help='Use Depth image for training (1/0)')
    parser.add_argument('--use-rgb', type=int, default=0, help='Use RGB image for training (0/1)')
    parser.add_argument('--split', type=float, default=0.9, help='Fraction of data for training (remainder is validation)')
    parser.add_argument('--ds-rotate', type=float, default=0.0,
                        help='Shift the start point of the dataset to use a different test/train split for cross validation.')
    parser.add_argument('--num-workers', type=int, default=8, help='Dataset workers')

    parser.add_argument('--batch-size', type=int, default=8, help='Batch size')
    parser.add_argument('--epochs', type=int, default=50, help='Training epochs')
    parser.add_argument('--batches-per-epoch', type=int, default=1000, help='Batches per Epoch')
    parser.add_argument('--val-batches', type=int, default=250, help='Validation Batches')

    # Logging etc.
    parser.add_argument('--description', type=str, default='', help='Training description')
    parser.add_argument('--outdir', type=str, default='output/models/', help='Training Output Directory')
    parser.add_argument('--logdir', type=str, default='tensorboard/', help='Log directory')
    parser.add_argument('--vis', action='store_true', help='Visualise the training process')

    args = parser.parse_args()
    return args

def train(epoch, train_data, batches_per_epoch, model):
    """
    Run one training epoch
    :param epoch: Current epoch
    :param net: Network
    :param device: Torch device
    :param train_data: Training Dataset
    :param optimizer: Optimizer
    :param batches_per_epoch:  Data batches to train on
    :param vis:  Visualise training progress
    :return:  Average Losses for Epoch
    """
    results = {
        'loss': 0,
        'losses': {
        }
    }
    #print(type(train_data))
    #print(len(train_data), len(train_data[0]), len(train_data[0][0]), len(train_data[0][0][0]))

    batch_idx = 0
    # Use batches per epoch to make training on different sized datasets (cornell/jacquard) more equivalent.
    while batch_idx < batches_per_epoch:
        for x, y in train_data:
            batch_idx += 1
            if batch_idx >= batches_per_epoch:
                break
            
            #x= tf.data.Dataset.from_tensor_slices(x)
            x = x.permute(0,2,3,1).data.numpy()

            y_off, y_w, y_angles = y
            y_angles = np.delete(y_angles,0,axis=3)

            

            with tf.GradientTape() as tape:
                y_pred = model(x)
                print(batch_idx)
                lossd = loss_function(y_pred[1], y_off,y_w,y_angles,y_pred[0])
            grads = tape.gradient(lossd, model.trainable_weights)
            model.optimizer.apply_gradients(zip(grads, model.trainable_weights))

            #results['loss'] = model.train_on_batch(x,y)

            if batch_idx % 100 == 0:
                logging.info('Epoch: {}, Batch: {}, Loss: {:0.4f}'.format(epoch, batch_idx, lossd['loss']))

            
            results['loss'] += lossd
            #for ln, l in lossd['losses']:
            #    if ln not in results['losses']:
            #        results['losses'][ln] = 0
            #    results['losses'][ln] += l


            # Display the images
            #if vis:
             #   imgs = []
              #  n_img = min(4, x.shape[0])
               # for idx in range(n_img):
                #    imgs.extend([x[idx,].numpy().squeeze()] + [yi[idx,].numpy().squeeze() for yi in y] + [
                 #       x[idx,].numpy().squeeze()] + [pc[idx,].detach().cpu().numpy().squeeze() for pc in lossd['pred'].values()])
                #gridshow('Display', imgs,
                 #        [(x.min().item(), x.max().item()), (0.0, 1.0), (0.0, 1.0), (-1.0, 1.0), (0.0, 1.0)] * 2 * n_img,
                  #       [cv2.COLORMAP_BONE] * 10 * n_img, 10)
                #cv2.waitKey(2)

    results['loss'] /= batch_idx
    for l in results['losses']:
        results['losses'][l] /= batch_idx

    return results

def validate( val_data, batches_per_epoch,model):
    """
    Run validation.
    :param net: Network
    :param device: Torch device
    :param val_data: Validation Dataset
    :param batches_per_epoch: Number of batches to run
    :return: Successes, Failures and Losses
    """
    

    results = {
        'correct': 0,
        'failed': 0,
        'loss': 0,
        'losses': {

        }
    }

    ld = len(val_data)
    
   
    batch_idx = 0
    while batch_idx < batches_per_epoch:
            for x, y in val_data:
                batch_idx += 1
                if batches_per_epoch is not None and batch_idx >= batches_per_epoch:
                    break
                
                x = x.permute(0,2,3,1).data.numpy()
                y_off, y_w, y_angles = y
                y_angles = np.delete(y_angles,0,axis=3)

                y_pred = model(x)
                lossd = loss_function(y_off,y_pred[1],y_w,y_angles,y_pred[0])
                results['loss'] += lossd/ld
                #for ln, l in lossd['losses']:
                #    if ln not in results['losses']:
                #        results['losses'][ln] = 0
                #    results['losses'][ln] += l/ld
                correct,failed = evaluate_grasp(y_off,y_pred[1],y_w,y_angles,y_pred[0])


                results['correct'] = correct

                results['failed'] = failed

    return results




def run():
    args = parse_args()
    # Vis window
    #if args.vis:
     #   cv2.namedWindow('Display', cv2.WINDOW_NORMAL)

    # Set-up output directories
    dt = datetime.datetime.now().strftime('%y%m%d_%H%M')
    net_desc = '{}_{}'.format(dt, '_'.join(args.description.split()))

    save_folder = os.path.join(args.outdir, net_desc)
    if not os.path.exists(save_folder):
        os.makedirs(save_folder)
    tb = tensorboardX.SummaryWriter(os.path.join(args.logdir, net_desc))

    # Load Dataset
    logging.info('Loading {} Dataset...'.format("cornell dataset"))
    train_dataset = GraspDatasetBase(start=0.0, end=0.9)
    train_data = torch.utils.data.DataLoader(
        train_dataset,
        batch_size=8,
        shuffle=True,
        num_workers=16
    )
    val_dataset = GraspDatasetBase(start=0.9, end=1.0)
    val_data = torch.utils.data.DataLoader(
        val_dataset,
        batch_size=8,
        shuffle=False,
        num_workers=args.num_workers
    )

    logging.info('Done')
    optimizer = Adam(0.001)
    model = make_model()
    model.compile(optimizer=optimizer, 
                         loss=loss_function)
    model.summary()

    logging.info('Done')

    best_iou = 0.0
    for epoch in range(args.epochs):
        logging.info('Beginning Epoch {:02d}'.format(epoch))
        #print(args.batches_per_epoch)
        train_results = train(epoch,train_data, args.batches_per_epoch, model)

        # Log training losses to tensorboard
        #tb.add_scalar('loss/train_loss', train_results['loss'], epoch)
        #for n, l in train_results['losses'].items():
        #    tb.add_scalar('train_loss/' + n, l, epoch)

        # Run Validation
        logging.info('Validating...')
        test_results = validate(val_data, args.val_batches,model)
        logging.info('%d/%d = %f' % (test_results['correct'], test_results['correct'] + test_results['failed'],
                                     test_results['correct']/(test_results['correct']+test_results['failed'])))
        

       # Log validation results to tensorbaord
        #tb.add_scalar('loss/IOU', test_results['correct'] / (test_results['correct'] + test_results['failed']), epoch)
        #tb.add_scalar('loss/val_loss', test_results['loss'], epoch)
        #SummaryWriter.close()
        #for n, l in test_results['losses']:
        #    tb.add_scalar('val_loss/' + n, l, epoch)

        # Save best performing network
        iou = test_results['correct'] / (test_results['correct'] + test_results['failed'])
        if iou > best_iou or epoch == 0 or (epoch % 10) == 0:
            model.save(os.path.join(save_folder, 'epoch_%02d_iou_%0.2f' % (epoch, iou)))

            best_iou = iou
            print(best_iou)

if __name__ == '__main__':
    run()

