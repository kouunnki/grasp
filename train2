import datetime
import os
import sys
import argparse
import logging

import cv2
import tensorflow as tf
from qkeras import *
from keras.layers import *
from keras.models import Model
from keras.optimizers import Adam
from keras import losses
import torch
import numpy
import torch.utils.data

import tensorboardX
from torch.utils.tensorboard import SummaryWriter
import matplotlib.pyplot as plt
from matplotlib.patches import Rectangle


from dataprocessing import GraspDatasetBase
from loss_function import *
from pretrain import *
from evaluation import *
logging.basicConfig(level=logging.INFO)

#tf.debugging.experimental.enable_dump_debug_info(
#    "/tmp/tfdbg2_logdir",
#    tensor_debug_mode="FULL_HEALTH",
#    circular_buffer_size=-1)

np.set_printoptions(threshold=np.inf)
torch.set_printoptions(threshold=float('inf'))
def parse_args():
    parser = argparse.ArgumentParser(description='Train GG-CNN')

    # Network
    parser.add_argument('--network', type=str, default='ggcnn', help='Network Name in .models')

    # Dataset & Data & Training
    parser.add_argument('--dataset', type=str, help='Dataset Name ("cornell" or "jaquard")')
    parser.add_argument('--dataset-path', type=str, help='Path to dataset')
    parser.add_argument('--use-depth', type=int, default=1, help='Use Depth image for training (1/0)')
    parser.add_argument('--use-rgb', type=int, default=0, help='Use RGB image for training (0/1)')
    parser.add_argument('--split', type=float, default=0.9, help='Fraction of data for training (remainder is validation)')
    parser.add_argument('--ds-rotate', type=float, default=0.0,
                        help='Shift the start point of the dataset to use a different test/train split for cross validation.')
    parser.add_argument('--num-workers', type=int, default=8, help='Dataset workers')

    parser.add_argument('--batch-size', type=int, default=8, help='Batch size')
    parser.add_argument('--epochs', type=int, default=100, help='Training epochs')
    parser.add_argument('--batches-per-epoch', type=int, default=70, help='Batches per Epoch')
    parser.add_argument('--val-batches', type=int, default=250, help='Validation Batches')

    # Logging etc.
    parser.add_argument('--description', type=str, default='', help='Training description')
    parser.add_argument('--outdir', type=str, default='output/models/', help='Training Output Directory')
    parser.add_argument('--logdir', type=str, default='tensorboard/', help='Log directory')
    parser.add_argument('--vis', action='store_true', help='Visualise the training process')

    args = parser.parse_args()
    return args

def train(epoch, train_data, batches_per_epoch, model,angle_head,CHV_head):
    """
    Run one training epoch
    :param epoch: Current epoch
    :param net: Network
    :param device: Torch device
    :param train_data: Training Dataset
    :param optimizer: Optimizer
    :param batches_per_epoch:  Data batches to train on
    :param vis:  Visualise training progress
    :return:  Average Losses for Epoch
    """
    results = {
        'loss': 0,
        'losses': {
        }
    }
    #print(type(train_data))
    #print(len(train_data), len(train_data[0]), len(train_data[0][0]), len(train_data[0][0][0]))

    batch_idx = 0
    # Use batches per epoch to make training on different sized datasets (cornell/jacquard) more equivalent.
    while batch_idx < batches_per_epoch:
        for x, y in train_data:
            batch_idx += 1
            if batch_idx >= batches_per_epoch:
                break
            #x= tf.data.Dataset.from_tensor_slices(x)
            x = x.permute(0,2,3,1).data.numpy()
            
            y_off, y_w, y_angles = y
            
            y_angles = np.delete(y_angles,0,axis=3)
            
            # 遍历每张图片

            

            with tf.GradientTape() as tape:
                y_pred = model(x)
                
                
                #plt.imshow(y_pred, cmap='hot',interpolation='nearest')
                #plt.savefig('./myplot.png')
                lossd,L_cls,L_reg = loss_function(y_off, y_pred[1],y_w,y_angles,y_pred[0])

            optimizer_angle = Adam(0.001)
            optimizer_CHV = Adam(0.0001)   
            grads_cls = tape.gradient(L_cls, angle_head.trainable_weights)
            #grads_norm = tf.linalg.global_norm(grads)

            # 设置裁剪阈值
            clip_norm = 1.0

            # 进行梯度裁剪
            clipped_grads_cls, _ = tf.clip_by_global_norm(grads_cls, clip_norm)
            optimizer_angle.apply_gradients(zip(clipped_grads_cls, angle_head.trainable_weights))
            
            grads_reg = tape.gradient(L_reg, CHV_head.trainable_weights)
            #grads_norm = tf.linalg.global_norm(grads)

            # 设置裁剪阈值
            clip_norm = 1.0

            # 进行梯度裁剪
            clipped_grads_reg, _ = tf.clip_by_global_norm(grads_reg, clip_norm)
            optimizer_angle.apply_gradients(zip(clipped_grads_reg, CHV_head.trainable_weights))

            #results['loss'] = model.train_on_batch(x,y)

            if batch_idx % 10 == 0:
               logging.info('Epoch: {}, Batch: {}, Loss: {:0.4f},Loss_cls: {:0.4f},Loss_reg: {:0.4f}'.format(epoch, batch_idx, lossd,L_cls,L_reg))
            

            
            results['loss'] += lossd
            #for ln, l in lossd['losses']:
            #    if ln not in results['losses']:
            #        results['losses'][ln] = 0
            #    results['losses'][ln] += l

            if epoch == 50 and batch_idx == 60:
                for i, array in enumerate(y_off):
                # 创建一个空白图像
                    image = np.zeros((320, 320))
                    fig, ax = plt.subplots()
                    ax.imshow(image)
                    

                # 遍历数组
                    for j in range(array.shape[0]):
                            for k in range(array.shape[1]):

                                if array[j, k].any():  # 如果该像素点对应的矩形存在
                                    
                                    
                                    x = j*8 - array[j, k, 0]  # 矩形左下角x坐标
                                    y = k*8 + array[j, k, 3]  # 矩形左下角y坐标
                                    
                                    x1 = j*8 - y_pred[1][i,j, k, 0]
                                    y1 = k*8 + y_pred[1][i,j, k, 3]
                                    width = array[j, k, 0] + array[j, k, 2]  # 矩形宽度
                                    
                                    height = array[j, k, 1] + array[j, k, 3]  # 矩形高度
                                    width1 = y_pred[1][i,j, k, 0] + y_pred[1][i,j, k, 2]
                                    height1 = y_pred[1][i,j, k, 1] + y_pred[1][i,j, k, 3]
                                    

                    # 创建矩形框并添加到当前图像中
                                    rect = Rectangle((x, y), width, height, linewidth=1, edgecolor='r', facecolor='none')
                                    rect1 = Rectangle((x1, y1), width1, height1, linewidth=1, edgecolor='g', facecolor='none')
                                    
                                    ax.add_patch(rect)
                                    ax.add_patch(rect1)

                
                    ax.set_title(f"Image {i+1}")
                    ax.axis('off')
                    fig.savefig(f"heatmap_{i+1}.png")
                    plt.close(fig)
                    #cv2.waitKey(2)

    results['loss'] /= batch_idx
    for l in results['losses']:
        results['losses'][l] /= batch_idx

    return results

def validate( val_data, batches_per_epoch,model):
    """
    Run validation.
    :param net: Network
    :param device: Torch device
    :param val_data: Validation Dataset
    :param batches_per_epoch: Number of batches to run
    :return: Successes, Failures and Losses
    """
    

    results = {
        'correct': 0,
        'failed': 0,
        'correct_iou':0,
        'failed_iou':0,
        'correct_angle':0,
        'failed_angle':0,
        'loss': 0,
        'losses': {

        }
    }

    ld = len(val_data)
    
   
    batch_idx = 0
    while batch_idx < batches_per_epoch:
            for x, y in val_data:
                batch_idx += 1
                if batches_per_epoch is not None and batch_idx >= batches_per_epoch:
                    break
                
                x = x.permute(0,2,3,1).data.numpy()
                y_off, y_w, y_angles = y
                y_angles = np.delete(y_angles,0,axis=3)

                y_pred = model(x)
                lossd,L_cls,L_reg = loss_function(y_off,y_pred[1],y_w,y_angles,y_pred[0])
                results['loss'] += lossd/ld
                #for ln, l in lossd['losses']:
                #    if ln not in results['losses']:
                #        results['losses'][ln] = 0
                #    results['losses'][ln] += l/ld
                correct,failed,correct_angle,correct_iou,failed_angle,failed_iou = evaluate_grasp(y_off,y_pred[1],y_angles,y_pred[0])


                results['correct'] = correct

                results['failed'] = failed
                results['correct_angle'] = correct_angle

                results['failed_angle'] = failed_angle
                results['correct_iou'] = correct_iou

                results['failed_iou'] = failed_iou
    return results




def run():
    args = parse_args()
    # Vis window
    #if args.vis:
     #   cv2.namedWindow('Display', cv2.WINDOW_NORMAL)

    # Set-up output directories
    dt = datetime.datetime.now().strftime('%y%m%d_%H%M')
    net_desc = '{}_{}'.format(dt, '_'.join(args.description.split()))

    save_folder = os.path.join(args.outdir, net_desc)
    if not os.path.exists(save_folder):
        os.makedirs(save_folder)
    tb = tensorboardX.SummaryWriter(os.path.join(args.logdir, net_desc))

    # Load Dataset
    logging.info('Loading {} Dataset...'.format("cornell dataset"))
    train_dataset = GraspDatasetBase(start=0.0, end=0.9)
    train_data = torch.utils.data.DataLoader(
        train_dataset,
        batch_size=128,
        shuffle=True,
        num_workers=16
    )
    val_dataset = GraspDatasetBase(start=0.9, end=1.0)
    val_data = torch.utils.data.DataLoader(
        val_dataset,
        batch_size=8,
        shuffle=False,
        num_workers=args.num_workers
    )

    logging.info('Done')
    optimizer_angle = Adam(0.001)
    optimizer_CHV = Adam(0.0001)
    model,angle_head,CHV_head = make_model(trainable=False)
    model.compile(optimizer=[optimizer_angle, optimizer_CHV],
              loss=loss_function)
    print(angle_head.summary())
    print(CHV_head.summary())

    logging.info('Done')

    best_iou = 0.0
    for epoch in range(args.epochs):
        logging.info('Beginning Epoch {:02d}'.format(epoch))
        #print(args.batches_per_epoch)
        #if epoch == 50:
        #    model.trainable=True
        #    optimizer = Adam(0.00001)
        #    model.compile(optimizer=optimizer, loss=loss_function)
        #    model.summary()
        train_results = train(epoch,train_data, args.batches_per_epoch, model,angle_head,CHV_head)

        # Log training losses to tensorboard
        tb.add_scalar('loss/train_loss', train_results['loss'].numpy(), epoch)
        for n, l in train_results['losses'].items():
            tb.add_scalar('train_loss/' + n, l, epoch)

        # Run Validation
        logging.info('Validating...')
        test_results = validate(val_data, 250,model)
        #logging.info('%d/%d = %f' % (test_results['correct'], test_results['correct'] + test_results['failed'],
        #                             test_results['correct']/(test_results['correct']+test_results['failed'])))
        

       # Log validation results to tensorbaord
        tb.add_scalar('loss/IOU', test_results['correct'] / (test_results['correct'] + test_results['failed']), epoch)
        #tb.add_scalar('loss/val_loss', test_results['loss'], epoch)
        #SummaryWriter.close()
        #for n, l in test_results['losses']:
        #    tb.add_scalar('val_loss/' + n, l, epoch)

        # Save best performing network
        iou = test_results['correct'] / (test_results['correct'] + test_results['failed'])
        accuracy_iou = test_results['correct_iou'] / (test_results['correct_iou'] + test_results['failed_iou'])
        accuracy_angles = test_results['correct_angle'] / (test_results['correct_angle'] + test_results['failed_angle'])
        print(iou)
        print(accuracy_angles)
        print(accuracy_iou)
        if iou > best_iou or epoch == 0 or (epoch % 10) == 0:
            #model.save(os.path.join(save_folder, 'epoch_%02d_iou_%0.2f' % (epoch, iou)))

            best_iou = iou
            print(best_iou)

if __name__ == '__main__':
    run()

